{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods in Data Science - Solution for Homework 3 - Fall 2024 - Wilmington College\n",
    "\n",
    "**Chapter 4: Exercises 5, 6, 7 and 8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Differences between LDA and QDA\n",
    "We now examine the differences between LDA and QDA.\n",
    "- (a) If the Bayes decision boundary is linear, do we expect LDA or\n",
    "QDA to perform better on the training set? On the test set?\n",
    "- (b) If the Bayes decision boundary is non-linear, do we expect LDA\n",
    "or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "- (c) In general, as the sample size n increases, do we expect the test\n",
    "prediction accuracy of QDA relative to LDA to improve, decline,\n",
    "or be unchanged? Why?\n",
    "- (d) True or False: Even if the Bayes decision boundary for a given\n",
    "problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible\n",
    "enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### (a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "- **Training set**: LDA should perform better or similarly to QDA on the training set. Since LDA assumes a linear decision boundary, it is well-suited for linear problems. QDA, being more flexible, might overfit the training data in this scenario.\n",
    "- **Test set**: LDA should also perform better or similarly to QDA on the test set. Since the true decision boundary is linear, QDA’s extra flexibility may lead to overfitting, which will result in worse generalization to the test set.\n",
    "\n",
    "#### (b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n",
    "\n",
    "- **Training set**: QDA should perform better on the training set because it can capture non-linear boundaries. LDA, on the other hand, will struggle because it assumes a linear boundary.\n",
    "- **Test set**: QDA should generally perform better on the test set because it can model non-linear boundaries, while LDA will suffer from a lack of flexibility in this scenario.\n",
    "\n",
    "#### (c) In general, as the sample size $ n $ increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n",
    "\n",
    "- **Improve**: As the sample size $ n $ increases, the relative test prediction accuracy of QDA compared to LDA is expected to improve. This is because QDA has more parameters and requires more data to accurately estimate the covariance matrices. With a larger sample size, QDA's flexibility will become beneficial without overfitting, and it will better model non-linear boundaries.\n",
    "\n",
    "#### (d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n",
    "\n",
    "- **False**: If the Bayes decision boundary is linear, LDA will generally perform better because it directly assumes a linear boundary, while QDA’s extra flexibility can lead to overfitting. Although QDA can model a linear boundary, it introduces unnecessary complexity by estimating different covariance matrices for each class, which can result in higher variance and worse performance, especially on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Logistic regression estimation\n",
    "Suppose we collect data for a group of students in a statistics class\n",
    "with variables $X_1$ = hours studied, $X_2$ = undergrad GPA, and Y=\n",
    "receive an A. We fit a logistic regression and produce estimated\n",
    "coefficient, $ \\hat{\\beta}_0 = -6 $, $ \\hat{\\beta}_1 = 0.05 $, and $ \\hat{\\beta}_2 = 1 $.\n",
    "- (a) Estimate the probability that a student who studies for 40 h and\n",
    "has an undergrad GPA of 3.5 gets an A in the class.\n",
    "- (b) How many hours would the student in part (a) need to study to\n",
    "have a 50 % chance of getting an A in the class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Given:\n",
    "- $ \\hat{\\beta}_0 = -6 $\n",
    "- $ \\hat{\\beta}_1 = 0.05 $\n",
    "- $ \\hat{\\beta}_2 = 1 $\n",
    "\n",
    "The logistic regression model is:  \n",
    "$$ P(Y = 1 | X_1, X_2) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2}} $$\n",
    "\n",
    "#### (a) Estimate the probability that a student who studies for 40 hours and has an undergrad GPA of 3.5 gets an A in the class.\n",
    "\n",
    "Substitute $ X_1 = 40 $ (hours studied) and $ X_2 = 3.5 $ (undergrad GPA) into the logistic regression equation:\n",
    "\n",
    "$$ \\hat{P}(Y = 1 | X_1 = 40, X_2 = 3.5) = \\frac{e^{-6 + (0.05 \\cdot 40) + (1 \\cdot 3.5)}}{1 + e^{-6 + (0.05 \\cdot 40) + (1 \\cdot 3.5)}} $$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$ \\hat{P}(Y = 1 | X_1 = 40, X_2 = 3.5) = \\frac{e^{-6 + 2 + 3.5}}{1 + e^{-6 + 2 + 3.5}} = \\frac{e^{-0.5}}{1 + e^{-0.5}} $$\n",
    "\n",
    "Use $ e^{-0.5} \\approx 0.6065 $:\n",
    "\n",
    "$$ \\hat{P}(Y = 1) = \\frac{0.6065}{1 + 0.6065} \\approx \\frac{0.6065}{1.6065} \\approx 0.377 $$\n",
    "\n",
    "So, the probability is approximately **0.38** or **38%**.\n",
    "\n",
    "#### (b) How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?\n",
    "\n",
    "We want $ P(Y = 1) = 0.5 $, which implies:\n",
    "\n",
    "$$ \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2}} = 0.5 $$\n",
    "\n",
    "At 50% probability, the logistic equation simplifies to:\n",
    "\n",
    "$$ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = 0 $$\n",
    "\n",
    "Substitute the given values for $ \\beta_0 $, $ \\beta_1 $, $ \\beta_2 $, and $ X_2 = 3.5 $:\n",
    "\n",
    "$$ -6 + 0.05 X_1 + 1(3.5) = 0 $$\n",
    "\n",
    "Simplify:\n",
    "\n",
    "$$ -6 + 0.05 X_1 + 3.5 = 0 $$\n",
    "\n",
    "$$ 0.05 X_1 = 2.5 $$\n",
    "\n",
    "$$ X_1 = \\frac{2.5}{0.05} = 50 $$\n",
    "\n",
    "So, the student would need to study for **50 hours** to have a 50% chance of getting an A.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Predicting stock dividend using Bayes' theorem\n",
    "\n",
    "Suppose that we wish to predict whether a given stock will issue a\n",
    "dividend this year (“Yes” or “No”) based on X, last year’s percent\n",
    "profit. We examine a large number of companies and discover that the\n",
    "mean value of X for companies that issued a dividend was $\\bar{X} = 10$,\n",
    "while the mean for those that didn’t was\n",
    "$\\bar{X} = 0$. In addition, the\n",
    "variance of X for these two sets of companies was\n",
    "$\\hat{\\sigma}^2 = 36$. Finally,\n",
    "80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue\n",
    "a divide this year given that its percentage profit was X = 4 last\n",
    "year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "We are given:\n",
    "- $ \\mu_{\\text{Yes}} = 10 $, $ \\mu_{\\text{No}} = 0 $\n",
    "- $ \\sigma^2 = 36 $ for both groups\n",
    "- Prior probabilities: $ P(\\text{Yes}) = 0.8 $, $ P(\\text{No}) = 0.2 $\n",
    "\n",
    "We want to calculate $ P(\\text{Yes} | X = 4) $ using Bayes’ theorem:\n",
    "\n",
    "$$ P(\\text{Yes} | X = 4) = \\frac{P(X = 4 | \\text{Yes}) P(\\text{Yes})}{P(X = 4 | \\text{Yes}) P(\\text{Yes}) + P(X = 4 | \\text{No}) P(\\text{No})} $$\n",
    "\n",
    "We know that $ X \\sim N(\\mu, \\sigma^2) $, so the likelihoods are:\n",
    "\n",
    "$$ P(X = 4 | \\text{Yes}) = \\frac{1}{\\sqrt{2\\pi(36)}} e^{-(4 - 10)^2 / 2(36)} $$\n",
    "$$ P(X = 4 | \\text{No}) = \\frac{1}{\\sqrt{2\\pi(36)}} e^{-(4 - 0)^2 / 2(36)} $$\n",
    "\n",
    "Let's compute these probabilities.\n",
    "\n",
    "- For $ P(X = 4 | \\text{Yes}) $:\n",
    "  $$\n",
    "  P(X = 4 | \\text{Yes}) = \\frac{1}{\\sqrt{72\\pi}} e^{-(6^2) / 72} = \\frac{1}{\\sqrt{72\\pi}} e^{-0.5} \\approx 0.06065\n",
    "  $$\n",
    "\n",
    "- For $ P(X = 4 | \\text{No}) $:\n",
    "  $$\n",
    "  P(X = 4 | \\text{No}) = \\frac{1}{\\sqrt{72\\pi}} e^{-(4^2) / 72} = \\frac{1}{\\sqrt{72\\pi}} e^{-0.222} \\approx 0.07887\n",
    "  $$\n",
    "\n",
    "Now, apply Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(\\text{Yes} | X = 4) = \\frac{0.06065 \\times 0.8}{0.06065 \\times 0.8 + 0.07887 \\times 0.2} \\approx \\frac{0.04852}{0.04852 + 0.01577} \\approx \\frac{0.04852}{0.06429} \\approx 0.755\n",
    "$$\n",
    "\n",
    "So, the probability that the company will issue a dividend given $ X = 4 $ is approximately **0.76** or **76%**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Comparing logistic regression and 1-nearest neighbors\n",
    "Suppose that we take a data set, divide it into equally-sized training\n",
    "and test sets, and then try out two different classification procedures.\n",
    "First we use logistic regression and get an error rate of 20 % on the\n",
    "training data and 30 % on the test data. Next we use 1-nearest neighbors (i.e. K = 1) and get an average error rate (averaged over both\n",
    "test and training data sets) of 18 %. Based on these results, which\n",
    "method should we prefer to use for classification of new observations?\n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "- **Logistic regression**: Training error = 20%, Test error = 30%\n",
    "- **1-nearest neighbors (1-NN)**: Average error = 18% (combined training and test)\n",
    "\n",
    "Since 1-NN is highly flexible, it likely has very low training error but high test error due to overfitting. Logistic regression, by contrast, has more stable test error and generalizes better. Given that 1-NN likely overfits (because $ k = 1 $ leads to very low bias but high variance), logistic regression is the better choice for generalizing to new observations, despite its higher average error compared to 1-NN's training performance.\n",
    "\n",
    "Hence, logistic regression should be preferred for classifying new observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
